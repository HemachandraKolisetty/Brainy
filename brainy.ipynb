{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45607155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import snntorch as snn\n",
    "from snntorch import spikegen\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "root_folder = \"./tactile_dataset/\"\n",
    "\n",
    "velocities = [30]\n",
    "\n",
    "num_textures = 12\n",
    "texture_names = [f\"texture_{i:02d}\" for i in range(1, num_textures + 1)]\n",
    "\n",
    "def load_and_merge_data(root, velocity, texture):\n",
    "    velocity_folder = os.path.join(root, f\"pickles_{velocity}\")\n",
    "    texture_folder = os.path.join(velocity_folder, texture)\n",
    "    \n",
    "    baro_file = os.path.join(texture_folder, \"full_baro.csv\")\n",
    "    imu_file = os.path.join(texture_folder, \"full_imu.csv\")\n",
    "    \n",
    "    baro_df = pd.read_csv(baro_file)\n",
    "    \n",
    "    imu_df = pd.read_csv(imu_file)\n",
    "    \n",
    "    imu_df['baro'] = baro_df['baro']\n",
    "        \n",
    "    return imu_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa026df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_list = []\n",
    "\n",
    "# for velocity in velocities:\n",
    "#     for texture in texture_names:\n",
    "#         df = load_and_merge_data(root_folder, velocity, texture).iloc[0:10000]\n",
    "#         if df.empty:\n",
    "#             print(f\"No data for Velocity: {velocity} mm/s, Texture: {texture}. Skipping...\")\n",
    "#             continue\n",
    "#         df_list.append(df)\n",
    "\n",
    "# merged_df = pd.concat(df_list)\n",
    "\n",
    "sliding_window_size=500\n",
    "for velocity in velocities:\n",
    "    for texture in texture_names:\n",
    "        merged_df = load_and_merge_data(root_folder, velocity, texture)\n",
    "        if merged_df.empty:\n",
    "            print(f\"No data for Velocity: {velocity} mm/s, Texture: {texture}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        total_samples = len(merged_df)\n",
    "\n",
    "        num_chunks = total_samples // sliding_window_size\n",
    "\n",
    "        truncated_df = merged_df.iloc[:num_chunks * sliding_window_size]\n",
    "\n",
    "        averaged_df = truncated_df.values.reshape(num_chunks, sliding_window_size, merged_df.shape[1]).mean(axis=1)\n",
    "\n",
    "        averaged_df = pd.DataFrame(averaged_df, columns=merged_df.columns)\n",
    "\n",
    "        df_list.append(averaged_df)\n",
    "\n",
    "final_merged_df = pd.concat(df_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a77188d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_merged_df[['baro','imu_ax', 'imu_ay', 'imu_az','imu_gx', 'imu_gy', 'imu_gz','imu_mx', 'imu_my', 'imu_mz']].values\n",
    "y = final_merged_df['Texture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "145d798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples collected: 90665\n",
      "Feature vector size: 10\n",
      "Unique textures: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Total samples collected: {X.shape[0]}\")\n",
    "print(f\"Feature vector size: {X.shape[1]}\")\n",
    "print(f\"Unique textures: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16f4fd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoded texture labels: [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "texture_classes = label_encoder.classes_\n",
    "num_classes = len(texture_classes)\n",
    "print(\"\\nEncoded texture labels:\", texture_classes)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de985b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# def count_classes(y, label_encoder):\n",
    "#     unique, counts = np.unique(y, return_counts=True)\n",
    "#     class_counts = dict(zip(label_encoder.inverse_transform(unique), counts))\n",
    "#     return class_counts\n",
    "\n",
    "# train_class_counts = count_classes(y_train, label_encoder)\n",
    "# test_class_counts = count_classes(y_test, label_encoder)\n",
    "\n",
    "# print(\"Training Set Class Distribution:\")\n",
    "# print(train_class_counts)\n",
    "# print(\"\\nTest Set Class Distribution:\")\n",
    "# print(test_class_counts)\n",
    "\n",
    "def poisson_encoding(features, num_steps, device='cpu'):\n",
    "\n",
    "    features_repeated = features.unsqueeze(1).repeat(1, num_steps, 1)\n",
    "    \n",
    "    rand_vals = torch.rand_like(features_repeated, device=device)\n",
    "    \n",
    "    spikes = (rand_vals < features_repeated).float()\n",
    "    \n",
    "    return spikes\n",
    "\n",
    "def encode_to_spike_train(features, num_steps, device='cpu'):\n",
    "    features_tensor = torch.tensor(features, dtype=torch.float32, device=device)\n",
    "\n",
    "    spike_trains = poisson_encoding(features_tensor, num_steps=num_steps, device=device)\n",
    "    \n",
    "    return spike_trains\n",
    "\n",
    "num_steps = 100\n",
    "train_spike_trains = encode_to_spike_train(X_train, num_steps)\n",
    "test_spike_trains = encode_to_spike_train(X_test, num_steps)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(train_spike_trains, y_train_tensor)\n",
    "test_dataset = TensorDataset(test_spike_trains, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b33b2642",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-16T21:31:55.171546Z",
     "iopub.status.busy": "2023-01-16T21:31:55.171181Z",
     "iopub.status.idle": "2023-01-16T21:31:55.768948Z",
     "shell.execute_reply": "2023-01-16T21:31:55.767637Z"
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1673831876642,
     "user": {
      "displayName": "Vinicius Prado da Fonseca",
      "userId": "07076031591394214642"
     },
     "user_tz": 210
    },
    "id": "-OpBEX94Jln0",
    "outputId": "60d65592-0eee-462c-b3b1-082af0be4245",
    "papermill": {
     "duration": 0.604373,
     "end_time": "2023-01-16T21:31:55.772089",
     "exception": false,
     "start_time": "2023-01-16T21:31:55.167716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.8571, Accuracy: 32.21%\n",
      "Epoch 2/20, Loss: 1.6680, Accuracy: 36.94%\n",
      "Epoch 3/20, Loss: 1.6356, Accuracy: 37.90%\n",
      "Epoch 4/20, Loss: 1.6165, Accuracy: 38.49%\n",
      "Epoch 5/20, Loss: 1.5998, Accuracy: 38.97%\n",
      "Epoch 6/20, Loss: 1.5868, Accuracy: 39.02%\n",
      "Epoch 7/20, Loss: 1.5771, Accuracy: 39.49%\n",
      "Epoch 8/20, Loss: 1.5710, Accuracy: 39.84%\n",
      "Epoch 9/20, Loss: 1.5636, Accuracy: 39.99%\n",
      "Epoch 10/20, Loss: 1.5597, Accuracy: 40.24%\n",
      "Epoch 11/20, Loss: 1.5536, Accuracy: 40.62%\n",
      "Epoch 12/20, Loss: 1.5493, Accuracy: 40.47%\n",
      "Epoch 13/20, Loss: 1.5458, Accuracy: 40.54%\n",
      "Epoch 14/20, Loss: 1.5418, Accuracy: 40.66%\n",
      "Epoch 15/20, Loss: 1.5389, Accuracy: 40.83%\n",
      "Epoch 16/20, Loss: 1.5351, Accuracy: 41.06%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "num_inputs = X_train.shape[1]\n",
    "num_hidden = 100\n",
    "num_outputs = num_classes\n",
    "beta = 0.9\n",
    "\n",
    "class SNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SNNModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "        self.lif1 = snn.Leaky(beta=beta)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.lif2 = snn.Leaky(beta=beta)\n",
    "        self.fc3 = nn.Linear(num_hidden, num_outputs)\n",
    "        self.lif3 = snn.Leaky(beta=beta)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk_rec = []\n",
    "        for step in range(x.size(1)):\n",
    "            input_step = x[:, step, :]\n",
    "            cur1 = self.fc1(input_step)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "            cur3 = self.fc3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "            spk_rec.append(spk3)\n",
    "        \n",
    "        out_spikes = torch.stack(spk_rec, dim=0)\n",
    "        return out_spikes\n",
    "\n",
    "net = SNNModel()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
    "\n",
    "def forward_pass(net, data):\n",
    "\n",
    "    spk_rec = net(data)\n",
    "    return spk_rec\n",
    "\n",
    "def train_model(net, train_loader, optimizer, criterion, num_epochs=20):\n",
    "\n",
    "    net.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for data, targets in train_loader:\n",
    "            data = data.float()\n",
    "            targets = targets.long()\n",
    "            spk_rec = forward_pass(net, data)\n",
    "            spk_sum = spk_rec.sum(dim=0)\n",
    "            loss = criterion(spk_sum, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            _, predicted = spk_sum.max(1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "        acc = total_correct / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss / len(train_loader):.4f}, Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "def evaluate_model(net, test_loader):\n",
    "    net.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data = data.float()\n",
    "            targets = targets.long()\n",
    "            spk_rec = forward_pass(net, data)\n",
    "            spk_sum = spk_rec.sum(dim=0)\n",
    "            _, predicted = spk_sum.max(1)\n",
    "            total_correct += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "    acc = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "num_epochs = 20\n",
    "train_model(net, train_loader, optimizer, criterion, num_epochs)\n",
    "\n",
    "evaluate_model(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb1dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.144066,
   "end_time": "2023-01-16T21:31:56.500051",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-01-16T21:31:46.355985",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
